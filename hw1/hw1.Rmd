---
title: Problem Set 1

author:
  - name: Carlos Enrique Lezama Jacinto
address:
  - address: Instituto Tecnológico Autónomo de México

corresponding_author:
  text: "My solutions to the first problem set in Advanced Microeconometrics (ECO -- 20513)."

pnas_type: pnasmathematics

bibliography: pnas-sample.bib
csl: pnas.csl

lineno: false

output:
  rticles::pnas_article:
      highlight: pygments
---
\newcommand{\ind}{\perp\!\!\!\!\perp}

```{r setup, include=FALSE}
if (!require("pacman"))
    install.packages("pacman")

pacman::p_load(rmarkdown,
               tidyverse)

options(digits = 4)

knitr::opts_chunk$set(
    cache = FALSE,
    echo = TRUE,
    fig.align = 'center',
    fig.height = 6,
    fig.width = 10,
    message = FALSE,
    out.extra = '',
    out.width = '85%',
    warning = FALSE
)

theme_set(theme_minimal()
)

op <- function(x, d = 4)
    sprintf(paste0("%1.", d, "f"), x)
```

# The Model {-}

Assume that the economic model generating the data for potential outcomes is of the form:

\begin{align*}
Y_1 &= \alpha + \varphi + U_1, \\
Y_0 &= \alpha + U_0,
\end{align*}

where $U_1$ and $U_0$ represent the unobservables in the potential outcome equations and $\varphi$ represents the benefit associated with the treatment ($D = 1$). Individuals decide whether or not to receive the treatment ($D = 1$ or $D = 0$) based on a latent variable $I$:

$$
I = Z \gamma + V,
$$

where $Z$ and $V$ represent observables and unobservables, respectively. Thus, we can define a binary variable $D$ indicating treatment status,

$$
D = \mathds{1}\left[ I \geq 0\right].
$$

Finally, we assume that the error terms in the model are not independent even conditioning on the observables, i.e. $U_1 \not\ind U_0 \not\ind V \mid Z$, but $(U_1, U_0, V) \ind Z$.

# Treatment Parameters

Let $\beta = Y_1 - Y_0 = \varphi + U_1 - U_0$ such that, in regression notation, $Y = \alpha + \beta D + \varepsilon$ where $\varepsilon = U_0$.

## Average Treatment Effect {-}

\begin{align*}
ATE &= E \left[ \beta \right] \\
&= E \left[ \varphi + U_1 - U_0 \right] \\
&= \varphi
\end{align*}

## Treatment on the Treated {-}

\begin{align*}
TT &= E \left[ \beta \mid D = 1 \right] \\
&= E \left[ \varphi + U_1 - U_0 \mid Z \gamma + V \geq 0 \right] \\
&= \varphi + E \left[ U_1 - U_0 \mid V \geq - Z \gamma \right]
\end{align*}

## Treatment on the Untreated {-}

\begin{align*}
TUT &= E \left[ \beta \mid D = 0 \right] \\
&= E \left[ \varphi + U_1 - U_0 \mid Z \gamma + V < 0 \right] \\
&= \varphi + E \left[ U_1 - U_0 \mid V < - Z \gamma \right]
\end{align*}

## Marginal Treatment Effect {-}

\begin{align*}
MTE &= E \left[ \beta \mid I = 0,\ V = v \right] \\
&= E \left[ \varphi + U_1 - U_0 \mid I = 0,\ V = v \right] \\
&= \varphi + E \left[ U_1 - U_0 \mid Z \gamma = - V,\ V = v \right]
\end{align*}

## Instrumental Variables {-}

$$
\hat{\beta}_{\text{IV}} (J(Z)) = \frac{\text{Cov}(J(Z), Y)}{\text{Cov}(J(Z), D)} \overset{p}{\longrightarrow} \beta
$$

## Ordinary Least Squares {-}

$$
\hat{\beta}_{\text{OLS}} = \frac{\text{Cov}(Y, D)}{\text{Var}(D)} \implies \hat{\beta}_{\text{OLS}} = \left( D^T D \right)^{-1} D^T Y
$$

## Local Average Treatment Effect {-}

\begin{align*}
LATE &= E \left[ \beta \mid D(z) = 0, D(z') = 1 \right] \\
&= E \left[ \beta \mid z \gamma < - V \leq z' \gamma \right] \\
&= \varphi + E \left[ U_1 - U_0 \mid - z' \gamma \leq V < - z \gamma \right]
\end{align*}

# Some closed form expressions

Suppose that the error terms in the model have the following structure:

\begin{align*}
U_1 &= \sigma_1 \epsilon, \\
U_0 &= \sigma_0 \epsilon, \\
V &= \sigma^*_V \epsilon, \\
\epsilon &\sim \mathcal{N} (0, 1).
\end{align*}

So, we can say that

\begin{align*}
TT &= \varphi + E \left[ U_1 - U_0 \mid V \geq - Z \gamma \right] \\
&= \varphi + E \left[ \epsilon (\sigma_1 - \sigma_0) \mid \epsilon \geq \frac{- Z \gamma}{\sigma^*_V} \right] \\
&= \varphi + (\sigma_1 - \sigma_0) E \left[ \epsilon \mid \epsilon \geq \frac{- Z \gamma}{\sigma^*_V} \right] \\
&= \varphi + (\sigma_1 - \sigma_0) \frac{\phi \left( - Z \gamma / \sigma^*_V \right)}{1 - \Phi \left( - Z \gamma / \sigma^*_V \right) },
\end{align*}

and

\begin{align*}
TUT &= \varphi + E \left[ U_1 - U_0 \mid V < - Z \gamma \right] \\
&= \varphi + E \left[ \epsilon (\sigma_1 - \sigma_0) \mid \epsilon < \frac{- Z \gamma}{\sigma^*_V} \right] \\
&= \varphi + (\sigma_1 - \sigma_0) E \left[ \epsilon \mid \epsilon < \frac{- Z \gamma}{\sigma^*_V} \right] \\
&= \varphi - (\sigma_1 - \sigma_0) \frac{\phi \left( - Z \gamma / \sigma^*_V \right)}{\Phi \left( - Z \gamma / \sigma^*_V \right) } \\
&= \varphi + (\sigma_0 - \sigma_1) \frac{\phi \left( - Z \gamma / \sigma^*_V \right)}{\Phi \left( - Z \gamma / \sigma^*_V \right) }.
\end{align*}

# Parametrization 1

Now, to add more structure to the problem, suppose that $Z = (1, Z_1, Z_2)$, and $\gamma = (\gamma_0, \gamma_1, \gamma_2)$. Also, suppose that

\begin{align*}
\gamma_0 = 0.2, && \gamma_1 = 0.3, && \gamma_2 = 0.1, \\
\sigma_1 = 0.012, && \sigma_0 = 0.05, && \sigma^*_V = 1, \\
\alpha = 0.02, && \varphi = 0.2.
\end{align*}

Finally,

\begin{align*}
Z_1 &\sim \mathcal{N}(-1, 9), \\
Z_2 &\sim \mathcal{N} (1, 9),
\end{align*}

where $Z_1 \ind Z_2$. Since we only observe either $Y_1$ or $Y_0$, the econometrician observes the outcome described as follows:

$$
Y = DY_1 + (1 - D)Y_0.
$$

\newpage

```{r}
set.seed(1234)
TOL <- 0.01

N <- 5000 # random sample observations

alpha <- 0.02
phi <- 0.2

g <- matrix(c(0.2, 0.3, 0.1))
sigma1 <- 0.012
sigma0 <- 0.05
sigmaV <- 1

eps <- rnorm(N)

U1 <- sigma1 * eps
U0 <- sigma0 * eps
V <- sigmaV * eps

Z <- cbind(rep(1, N),
           rnorm(N, -1, 3),
           rnorm(N, 1, 3))

I <- (Z %*% g) + V
D <- ifelse(I >= 0, 1, 0)

Y1 <- alpha + phi + U1
Y0 <- alpha + U0

Y <- (D * Y1) + ((1 - D) * Y0)

df <- data.frame('beta' = Y1 - Y0,
                 'decision' = D,
                 'y1' = Y1,
                 'y0' = Y0,
                 'net utility' = I)

ATE <- df %>%
    pull(beta) %>% 
    mean()

TT <- df %>%
    filter(decision == 1) %>%
    pull(beta) %>%
    mean()

TUT <- df %>%
    filter(decision == 0) %>%
    pull(beta) %>%
    mean()

MTE <- df %>%
    filter(abs(net.utility - V) <= TOL) %>%
    pull(beta) %>%
    mean()
```

```{r, echo=FALSE}
blue <- '#1111ff'
pink <- '#ff11ff'
red <- '#ff1111'

plt.1 <- ggplot(df, aes(x = beta, y = ..density..)) +
    geom_density(alpha = 0.4, fill = pink) +
    geom_vline(xintercept = ATE, linetype = 'dashed') +
    geom_text(aes(x = ATE - 0.003, y = 5, label = 'ATE'), angle = 90) +
    geom_vline(xintercept = TT,
               linetype = 'dashed',
               colour = blue) +
    geom_text(aes(x = TT - 0.003, y = 5, label = 'TT'), angle = 90) +
    geom_vline(xintercept = TUT,
               linetype = 'dashed',
               colour = blue) +
    geom_text(aes(x = TUT + 0.003, y = 5, label = 'TUT'), angle = 90) +
    geom_vline(xintercept = MTE,
               linetype = 'dashed',
               colour = red) +
    geom_text(aes(x = MTE + 0.003, y = 5, label = 'MTE'), angle = 90) +
    labs(
        title = 'Distribution of Gains',
        x = expression(y[1] - y[0]),
        y = NULL,
        caption = paste0(
            'ATE = ',
            op(ATE),
            ', TT = ',
            op(TT),
            ', TUT = ',
            op(TUT),
            ', MTE (for V = 0) = ',
            op(MTE)
        )
    )

plt.1
```

# Parametrization 2

```{r}
LATE <- function(param, old, new) {
    Z.old <- Z
    Z.new <- Z
    
    Z.old[, param] <- old
    Z.new[, param] <- new
    
    I.old <- (Z.old %*% g) + V
    D.old <- ifelse(I.old >= 0, 1, 0)
    
    I.new <- (Z.new %*% g) + V
    D.new <- ifelse(I.new >= 0, 1, 0)
    
    test <- ifelse((D.old == 0) & (D.new == 1), 1, 0)
    
    temp.df <- data.frame(
        'beta' = Y1 - Y0,
        'test' = test
    )
    
    r <- temp.df %>%
        filter((test == 1)) %>%
        pull(beta) %>%
        mean()
    
    return(r)
}

regress <- function(beta) {
    intercept <- mean(Y) - (beta * mean(D))
    intercept + (beta * D)
}
```

## $LATE(z_1 = -2, z'_1 = 1)$

```{r}
LATE.1 <- LATE(2, -2, 1)
```

$LATE(z_1 = -2, z'_1 = 1) = `r LATE.1`$.

## $LATE(z_2 = 0, z'_2 = 2)$

```{r}
LATE.2 <- LATE(3, 0, 2)
```

$LATE(z_2 = 0, z'_2 = 2) = `r LATE.2`$.

## $IV(Z_1)$

```{r}
beta.IV.Z1 <- (cov(Z[, 2], Y) / cov(Z[, 2], D))[1, 1]
Y.IV.Z1 <- regress(beta.IV.Z1)
```

$\hat{\beta}_{\text{IV}} (Z_1) = `r beta.IV.Z1`$.

## $IV(Z_2)$

```{r}
beta.IV.Z2 <- (cov(Z[, 3], Y) / cov(Z[, 3], D))[1, 1]
Y.IV.Z2 <- regress(beta.IV.Z2)
```

$\hat{\beta}_{\text{IV}} (Z_2) = `r beta.IV.Z2`$.

## $OLS$

```{r}
beta.OLS <- (cov(D, Y) / var(D))[1, 1]
Y.OLS <- regress(beta.OLS)
```

$\hat{\beta}_{\text{OLS}} = `r beta.OLS`$.

# GMM

Using $Z = (Z_1, Z_2)$ as instruments and $\displaystyle \hat{V}_0 = \frac{1}{n} \sum_{i = 1}^n z_i' z_i$ as the weighting matrix, we can estimate our model by the generalized method of moments.

Hence, we can obtain

$$
\hat{\beta}_{\text{GMM}} = \left( D' Z V_0^{-1} Z' D \right)^{-1} D' Z V_0^{-1} Z' Y.
$$

Note that we can rewrite $\displaystyle \hat{V}_0 = \frac{1}{n} Z' Z$.

Thus,

```{r}
subset.Z <- Z[, -1]

V0 <- (1 / N) * (t(subset.Z) %*% subset.Z)

beta.GMM <-
    (
        solve(t(D) %*% subset.Z %*% solve(V0) %*% t(subset.Z) %*% D) %*%
            t(D) %*% subset.Z %*% solve(V0) %*% t(subset.Z) %*% Y
    )[1, 1]

Y.GMM <- regress(beta.GMM)
```

| Estimator| Estimate | Standard Error |
|:---|:---|:---|
| $\hat{\beta}_{\text{GMM}}$ | `r beta.GMM` | `r sd(Y - Y.GMM)` |
| $\hat{\beta}_{\text{IV}} (Z_1)$ | `r beta.IV.Z1` | `r sd(Y - Y.IV.Z1)` |
| $\hat{\beta}_{\text{IV}} (Z_2)$ | `r beta.IV.Z2` | `r sd(Y - Y.IV.Z2)` |
| $\hat{\beta}_{\text{OLS}}$ | `r beta.OLS` | `r sd(Y - Y.OLS)` |

# Likelihood Function

We know that

\begin{align*}
Pr\left(D = 1 \mid Z\right) &= Pr\left(\gamma Z + V \geq 0 \right) \\
&= Pr\left( \frac{-Z\gamma}{\sigma^*_V} \leq \epsilon \right) \\
&= 1 - Pr\left( \epsilon \leq \frac{-Z\gamma}{\sigma^*_V} \right) \\
&= 1 - \Phi \left( \frac{-Z\gamma}{\sigma^*_V} \right),
\end{align*}

and

\begin{align*}
Pr\left(D = 0 \mid Z\right) &= Pr\left(\gamma Z + V < 0 \right) \\
&= Pr\left( \epsilon \leq \frac{-Z\gamma}{\sigma^*_V} \right) \\
&= \Phi \left( \frac{-Z\gamma}{\sigma^*_V} \right).
\end{align*}

Hence,

\begin{align*}
\mathcal{L} &= \prod_{d_i = 0} \left[ \Phi \left( \frac{-Z\gamma}{\sigma^*_v} \right) \right] \prod_{d_i = 1} \left[ 1-  \Phi \left( \frac{-Z\gamma}{\sigma^*_v} \right) \right] \\
&= \prod_{i} \left[ \Phi \left( \frac{-Z\gamma}{\sigma^*_v} \right) \right]^{1 - d_i} \left[ 1-  \Phi \left( \frac{-Z\gamma}{\sigma^*_v} \right) \right]^{d_i},
\end{align*}

and

\begin{align*}
\ell &= \log\mathcal{L} \\
&= \sum_{i} (1 - d_i) \log \left( \Phi \left( \frac{-Z\gamma}{\sigma^*_V} \right) \right) \\
&+ \sum_{i} d_i \log \left( 1 - \Phi \left( \frac{-Z\gamma}{\sigma^*_V} \right) \right) \\
&= \sum_{i} (1 - d_i) \log \left( \Phi \left( \frac{-\gamma_0 - z_1\gamma_1 - z_2\gamma_2}{\sigma^*_V} \right) \right) \\
&+ \sum_{i} d_i \log \left( 1 - \Phi \left( \frac{-\gamma_0 - z_1\gamma_1 - z_2\gamma_2}{\sigma^*_V} \right) \right).
\end{align*}

# Discussion

# Maximum Likelihood

<!-- Leave these lines as they are at the end of your .Rmd file to ensure placement of methods & acknowledgements sections before the references-->
\showmatmethods
\showacknow
\pnasbreak
